---
title: "chapter4"
author: "Yuwen Pang"
date: "19/11/2021"
output: html_document
---

# chapter 4 clustering and classification
```{r}
date()
```

## The diary for this chapter, clustering and classification
## Analysis exercises
- 1 Load the Boston data from the MASS package
```{r}
# 1 Load the Boston data from the MASS package
# The full R scripts gave in my github page the 'Ex4_cluster and classification.R'
```

- 2 Show a graphical overview of the data
```{r, fig.width=10, fig.hight=4, message = FALSE}
# Load the library
library(MASS)
library(ggplot2)
library(GGally)
# Load the Boston data from the MASS package
data("Boston") #506*14
# Show a graphical overview of the data
pairs(Boston)
p <- ggpairs(Boston,
             lower = list(combo = wrap("facethist", bins = 20)))
p
```

- 3 Standardize the dataset and print out summaries of the scaled data
- Now all column have been scaled
```{r, echo=FALSE}
Boston_stand <- scale(Boston)
summary(Boston_stand)

```

- 4 Fit the linear discriminant analysis
```{r, echo=FALSE}
Boston_stand <- as.data.frame(Boston_stand)
# create a quantile vector of crim
bins <- quantile(Boston_stand$crim)
# create a categorical variable 'crime'
crime <- cut(Boston_stand$crim, breaks = bins, include.lowest = TRUE,
             lables = c("low", "med_low", "med_high", "high"))
table(crime)

# remove original crim from the dataset
Boston_stand <- dplyr::select(Boston_stand, -crim)

# add the new categorical value to scaled data
Boston_stand <- data.frame(Boston_stand, crime)

# divide the dataset to train and test sets
# number of rows in the Boston dataset 
n <- nrow(Boston_stand) #506
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- Boston_stand[ind,]
# create test set 
test <- Boston_stand[-ind,]

# Linear Discriminant analysis
# Fit the linear discriminant analysis on the train set
lda.fit <- lda(crime ~. , data = train)
# print the lda.fit object
lda.fit
```

```{r, echo=FALSE}
# Draw the LDA (bi)plot
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

- 6 Save the crime categorie and comments on results
(1) the overall accuracy;0.67%
(2) 'high' category had the highest accuracy rate
```{r, echo=FALSE}
correct_classes <- test$crime
# 6 and then remove the categorical crime variable from the test dataset.
test <- dplyr::select(test, -crime)

# predict the classes with the LDA model on the test data
lda.pred <- predict(lda.fit, newdata = test)
# Cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
print(mean(correct_classes==lda.pred$class)) # the accuracy:0.6764706

```

- 7 re-paractice the k-means clustering
(1) identify the optimal number of clusters by silhouette width, k=2
(2) implement k-means algorithm
(3) visualize the clusters
(4) interpret the results: 177 for cluster 1; 329 for cluster 2.
```{r, echo=FALSE}
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization

data(Boston)
scl_boston <- scale(Boston)

## identify the optimal k number
fviz_nbclust(scl_boston, kmeans, method = "silhouette")   # k=2
# k-means algorithm
kmeans_bos <- kmeans(scl_boston, centers = 2)
#  Visualize the clusters
fviz_cluster(kmeans_bos, data = scl_boston, ellipse.type = 'convex', main = "kmeans of Boston")
# interpret the results
table(kmeans_bos$cluster)
# 177 for cluster 1; 329 for cluster 2.
```

- 8 Bonus
(1) Perform k-means on the original Boston data 
(2) with some reasonable number of clusters
```{r, echo=FALSE}
fviz_nbclust(scl_boston, kmeans, method = "gap_stat")   # k=4

# the identified optiaml k number is 4
kmeans_bosv2 <- kmeans(scl_boston, centers = 4)

# perform LDA using the clusters as target classes
clas <- as.factor(kmeans_bosv2$cluster)

# add the k-means cluster to scaled Boston datasets
reBoston <- data.frame(scl_boston, clas)
# explore the new data set
str(reBoston)

# fixed lda
fix.lda <- lda(clas ~. , data = reBoston[,-1])
# Visualize the new rda results with a biplot
plot(fix.lda, dimen = 2, col = as.numeric(clas), pch = as.numeric(clas))
lda.arrows(fix.lda, myscale = 1)

```

-9 No Super-Bonus

## Section 2 Data wrangling for the next weekâ€™s data, human.csv
- 10 the after-procsses human.csv
```{r, echo=FALSE}
human <- read.csv('data/human.csv')
head(human,n=6)
```



